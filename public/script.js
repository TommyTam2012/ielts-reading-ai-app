console.log("ğŸŸ¢ script.js loaded successfully");

const responseBox = document.getElementById("responseBox");
const questionInput = document.getElementById("questionInput");
const historyList = document.getElementById("historyList");
const micBtn = document.getElementById("micBtn");

const translationBox = document.createElement("div");
translationBox.id = "chineseTranslation";
translationBox.style.marginTop = "10px";
translationBox.style.fontSize = "0.95em";
translationBox.style.color = "#333";
responseBox.insertAdjacentElement("afterend", translationBox);

let currentExamId = "";

function setExam(examId) {
  currentExamId = examId;
  const pdfUrl = `/exam/IELTS/${examId}.pdf`;
  window.open(pdfUrl, "_blank");
  console.log(`ğŸ“˜ Exam set to ${examId}`);
}

function clearHistory() {
  historyList.innerHTML = "";
  console.log("ğŸ§¹ History cleared");
}

async function submitQuestion() {
  const question = questionInput.value.trim();
  if (!question || !currentExamId) {
    alert("âš ï¸ è«‹é¸æ“‡è©¦å·ä¸¦è¼¸å…¥å•é¡Œ");
    return;
  }

  responseBox.textContent = "æ­£åœ¨åˆ†æä¸­ï¼Œè«‹ç¨å€™...";
  translationBox.textContent = "";

  const instruction = `
You are an IELTS Academic Reading instructor. The student is asking about test ${currentExamId.toUpperCase()}.

If they ask about a specific question (e.g., Q5 or paragraph C), find the correct answer from the images provided.

After providing the answer:
1. State which **paragraph** or **section** contains the answer.
2. Quote or paraphrase the **exact sentence** that proves the answer.
3. Be detailed but clear â€” this is for exam training.

Only summarize the passage if the student requests it explicitly.
`;

  const maxPages = 13;
  let imageMessages = [
    { type: "text", text: instruction },
    { type: "text", text: question }
  ];

  const baseUrl = `${window.location.origin}/exam/IELTS/${currentExamId}_page`;

  let availablePages = 0;

  for (let i = 1; i <= maxPages; i++) {
    const url = `${baseUrl}${i}.png`;
    try {
      const res = await fetch(url, { method: "HEAD" });
      if (res.ok) {
        imageMessages.push({
          type: "image_url",
          image_url: { url }
        });
        console.log(`âœ… Found image: ${url}`);
        availablePages++;
      } else {
        console.warn(`âš ï¸ Skipped: ${url} (404)`);
      }
    } catch (err) {
      console.warn(`âš ï¸ Error fetching: ${url}`, err);
    }
  }

  if (availablePages === 0) {
    responseBox.textContent = "âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åœ–ç‰‡é é¢ï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦ä¸Šå‚³æ­£ç¢ºã€‚";
    return;
  }

  fetch("/api/analyze", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ prompt: question, messages: imageMessages })
  })
    .then(async res => {
      const text = await res.text();
      try {
        return JSON.parse(text);
      } catch (err) {
        console.error("âŒ Server returned non-JSON:", text);
        throw new Error("GPT returned non-JSON response");
      }
    })
    .then(data => {
      const answer = data.response || "âŒ ç„¡æ³•ç²å–è‹±æ–‡å›ç­”ã€‚";
      const translated = data.translated || "âŒ ç„¡æ³•ç¿»è­¯ç‚ºä¸­æ–‡ã€‚";

      responseBox.textContent = answer;
      translationBox.textContent = `ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç¿»è­¯ï¼š${translated}`;
      addToHistory(question, `${answer}<br><em>ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç¿»è­¯ï¼š</em>${translated}`);
    })
    .catch(err => {
      responseBox.textContent = "âŒ ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œé‡è©¦ã€‚";
      console.error("GPT error:", err);
    });

  questionInput.value = "";
}

function addToHistory(question, answer) {
  const li = document.createElement("li");
  li.innerHTML = `<strong>å•ï¼š</strong>${question}<br/><strong>ç­”ï¼š</strong>${answer}`;
  historyList.prepend(li);
}

function detectLang(text) {
  return /[\u4e00-\u9fa5]/.test(text) ? "zh-CN" : "en-GB";
}

function getVoiceForLang(lang) {
  const voices = speechSynthesis.getVoices();
  if (lang === "zh-CN") {
    return voices.find(v => v.lang === "zh-CN") || voices.find(v => v.name.includes("Google æ™®é€šè¯ å¥³å£°"));
  } else {
    return voices.find(v => v.lang === "en-GB") || voices.find(v => v.name.includes("Google UK English Female"));
  }
}

function speakMixed(text) {
  const segments = text.split(/(?<=[ã€‚.!?])/).map(s => s.trim()).filter(Boolean);
  let index = 0;

  function speakNext() {
    if (index >= segments.length) return;
    const segment = segments[index++];
    const lang = detectLang(segment);
    const utter = new SpeechSynthesisUtterance(segment);
    utter.lang = lang;
    utter.voice = getVoiceForLang(lang);
    utter.rate = 1;
    utter.onend = speakNext;
    speechSynthesis.speak(utter);
  }

  speechSynthesis.cancel();
  speakNext();
}

document.getElementById("ttsBtn")?.addEventListener("click", () => {
  const english = responseBox.textContent.trim();
  const chinese = translationBox.textContent.replace(/^ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç¿»è­¯ï¼š/, "").trim();
  speakMixed(`${english} ${chinese}`);
});

document.getElementById("stopTTSBtn")?.addEventListener("click", () => {
  speechSynthesis.cancel();
  console.log("ğŸ›‘ TTS playback stopped");
});

// ğŸ¤ Manual press-and-hold mic input (no auto-timeout)
if (window.SpeechRecognition || window.webkitSpeechRecognition) {
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  const recognition = new SpeechRecognition();
  recognition.lang = "zh-CN";
  recognition.continuous = false;
  recognition.interimResults = false;

  micBtn.addEventListener("mousedown", () => {
    recognition.start();
    micBtn.textContent = "ğŸ¤ éŒ„éŸ³ä¸­... (è«‹æŒçºŒæŒ‰ä½)";
  });

  micBtn.addEventListener("mouseup", () => {
    recognition.stop();
    micBtn.textContent = "ğŸ¤ èªéŸ³æå•";
  });

  micBtn.addEventListener("mouseleave", () => {
    recognition.stop();
    micBtn.textContent = "ğŸ¤ èªéŸ³æå•";
  });

  micBtn.addEventListener("touchstart", () => {
    recognition.start();
    micBtn.textContent = "ğŸ¤ éŒ„éŸ³ä¸­... (è«‹æŒçºŒæŒ‰ä½)";
  });

  micBtn.addEventListener("touchend", () => {
    recognition.stop();
    micBtn.textContent = "ğŸ¤ èªéŸ³æå•";
  });

  recognition.onresult = (event) => {
    const spoken = event.results[0][0].transcript;
    questionInput.value = spoken;
    submitQuestion();
  };

  recognition.onerror = (event) => {
    alert("ğŸ¤ éŒ„éŸ³å¤±æ•—ï¼Œè«‹é‡è©¦ã€‚");
    console.error("Mic error:", event.error);
  };
}

window.submitQuestion = submitQuestion;
window.setExam = setExam;
window.clearHistory = clearHistory;
